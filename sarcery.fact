"""Parse text files generated by sysstat (sar)

This is intended to run as a "fact" script and to provide Ansible with
information (via JSON) for system monitoring. To this end, the script
will isolate a default set of counters, and will read one week's worth
of /var/log/sa/sardd files (where the dd parameter indicates the
day-of-month, and the seven most recent _full days_ -- not including
today -- are selected and parsed).
"""


import json
import os
import os.path
import re
import sys


class ParsedSection:
    """Structure which represents a blank-delimited section in the file

    For example, this is a section:

    12:00:00 AM     wtps     rtps
    12:10:00 AM      100      200
    12:20:00 AM      300      400
    Average:         200      300

    There are three types of section:
    * Descriptive/comment section, like the one at the beginning of the
      file with OS version and hostname; this isn't actually
      represented by this datatype, but is stored as NoneType
    * Standalone section without identifiers (like above), for which
      has_identifiers and identifier_field_name are left as None
    * Standalone section with identifiers, where one of the fields
      indicates the subsystem for which the counter is applicable:

    12:00:00 AM     CPU     %usr
    12:10:00 AM     all      1.1
    12:10:00 AM       0      2.2
    12:10:00 AM       1      3.3
    Average:        all      1.1
    Average:          0      2.2
    Average:          1      3.3

    Each of the two "standalone" section types can also be followed or
    preceded by another section with *identical headers*, and typically
    with a continuation of the timestamp series. Combining these is
    done after-the-fact, by a different part of the script.
    """

    def __init__(self):
        self.content = None
        self.headers = None
        self.has_identifiers = None
        self.identifier_field_name = None


def separate_by_identifier(in_dict, identifier_field_name):
    """Split a series of counters by identifier (e.g., eth0 vs all)
    
    Arguments:
    in_dict -- the original series of counters
    identifier_field_name -- the field name to identify the subsystem
                             (e.g., CPU) which the counter describes
    
    For example, given:
    time0     CPU      %usr
    time1       0       1.0
    time1       1       2.0
    time1     all       1.5
    time2       0       2.0
    time2       1       4.0
    time2     all       3.0

    transform the data set into:
    time0     0::%usr    1::%usr    all::%usr
    time1         1.0        2.0          1.5
    time2         2.0        4.0          3.0
    """

    all_identifiers = set()
    for field in in_dict[identifier_field_name]:
        all_identifiers.add(field)

    # Make one set of headers for each identifier (e.g., eth0::rxerr/s
    # and all::rxerr/s); but don't separate the timestamps, rather
    # deduplicate and copy them
    out_dict = {'timestamp': []}
    for timestamp in in_dict['timestamp']:
        if len(out_dict['timestamp']) == 0 or out_dict['timestamp'][-1] != timestamp:
            out_dict['timestamp'].append(timestamp)
    for field in in_dict.keys():
        # There is no place for the identifier field in out_dict
        if field != identifier_field_name and field != 'timestamp':
            for identifier in all_identifiers:
                out_dict[identifier + "::" + field] = []
            for identifier, value in zip(in_dict[identifier_field_name], in_dict[field]):
                out_dict[identifier + "::" + field].append(value)

    return out_dict


def parse_file_content(lines):
    """Parse the contents of a sar text file into combined sections
    
    The definition of a "section" is given in the ParsedSection class
    docstring. This function applies three transformations to a raw
    file (e.g., from /var/log/sa/sar*):
    1. Parse each section independently, while checking for blank lines
       (e.g., between sections with CPU info vs. with network info)
    2. Combine consecutive sections, when the headers are the same
       (it isn't clear why sar will split sections like this...)
    3. Identify (combined) sections which have subsystem "identifiers,"
       signifying different CPUs or network devices, etc.; then apply
       the logic in the "separate" function (above) to split by
       identifiers for easier analysis
    """

    fileheader_re = re.compile(r'^Linux.*')
    measurement_re = re.compile(r'^((?:[0-9:]+ ?(?:AM|PM)?)|Average:)[ \t]+([^ \t].*)$')
    blank_re = re.compile(r'^[ \t]*$')
    # TODO: better, more flexible way to detect "identifiers"
    # (heuristics? maybe any field name that is all caps?)
    sections_with_identifiers = frozenset(['CPU', 'DEV', 'IFACE'])

    sections = []
    this_section = None

    for line in lines:
        if fileheader_re.match(line):
            # Completely ignore the fileheader
            next

        if blank_re.match(line):
            # Blank line indicates a new section
            sections.append(this_section)
            this_section = ParsedSection()

        m = measurement_re.match(line)
        if m != None:
            if this_section.content == None:
                # Haven't found the column headers yet; do that now
                # 'timestamp' is a special field
                this_section.content = {'timestamp': []}
                this_section.headers = m.group(2).split()

                # There are times when multiple consecutive sections actually show the
                # same data (e.g., one CPU section, then a blank line, then another CPU
                # section); combine these into one by keeping track of last section
                if (len(sections) > 0 and sections[-1] != None and
                    set(sections[-1].headers) == set(this_section.headers)):
                    # Revisit this section; it is a continuation of the last # one
                    this_section = sections.pop()
                else:
                    # Generate a new blank section_dict
                    for header in this_section.headers:
                        this_section.content[header] = []

                if this_section.headers[0] in sections_with_identifiers:
                    this_section.identifier_field_name = this_section.headers[0]
                    this_section.has_identifiers = True
                else:
                    this_section.has_identifiers = False
            else:
                this_section.content['timestamp'].append(m.group(1))
                for k, v in zip(this_section.headers, m.group(2).split()):
                    if this_section.has_identifiers and k in sections_with_identifiers:
                        # Section sub-identifiers are strings
                        this_section.content[k].append(v)
                    else:
                        # All other values are numeric measurements
                        this_section.content[k].append(float(v))

    # Don't forget the last section, if the file didn't end with a blank line
    if this_section.content != None:
        sections.append(this_section)

    processed_sections = []
    for section in sections:
        # Remove any content-free sections (like with the fileheader)
        if section == None:
            pass
        elif section.has_identifiers == True:
            # Separate out by identifier
            processed_sections.append(separate_by_identifier(section.content, section.identifier_field_name))
        else:
            # Just store the content for non-identifier sections
            processed_sections.append(section.content)

    return processed_sections


def isolate_desired_fields(parsed_data, fields):
    """Analyze averages and maximums for the given field names

    For each field, also consider any "identifier"-prefixed variants.
    For example, requesting the "rtps" field will simply retrieve data
    from the "rtps" column, which represents system-wide read IOPS.
    Conversely, requesting the "%usr" field will retrieve data for
    multiple fields based on the number of CPUs: 0::%usr, 1::%usr, and
    all::%usr, perhaps.
    """

    # xyz, but also eth0::xyz (*::xyz)
    matched_fields = []
    for desired_field in fields:
        for section in parsed_data:
            for section_field in section.keys():
                if desired_field == section_field or section_field.endswith("::"+desired_field):
                    matched_fields.append(section_field)

    averages = {}
    for matched_field in matched_fields:
        for section in parsed_data:
            if matched_field in section.keys():
                for timestamp, measurement in zip(section['timestamp'], section[matched_field]):
                    if timestamp == 'Average:':
                        averages[matched_field] = measurement

    minimums = {}
    maximums = {}
    for matched_field in matched_fields:
        for section in parsed_data:
            if matched_field in section.keys():
                minimum_timestamp, minimum_measurement = None, None
                maximum_timestamp, maximum_measurement = None, None
                for timestamp, measurement in zip(section['timestamp'], section[matched_field]):
                    if minimum_measurement == None or measurement < minimum_measurement:
                        minimum_timestamp, minimum_measurement = timestamp, measurement
                    if maximum_measurement == None or measurement > maximum_measurement:
                        maximum_timestamp, maximum_measurement = timestamp, measurement
                minimums[matched_field] = {'timestamp': minimum_timestamp, 'measurement': minimum_measurement}
                maximums[matched_field] = {'timestamp': maximum_timestamp, 'measurement': maximum_measurement}

    return {'averages': averages, 'minimums': minimums, 'maximums': maximums}


if __name__ == "__main__":
    # By default, read the latest seven entries from the sysstat directory
    # (note that today is not included, because no sarXX ASCII file is saved
    #  until the end of the day)
    sar_path = "/var/log/sa"
    sar_files = []
    for sar_file in os.listdir(sar_path):
        sar_files.append({
            'name': sar_file,
            'path': os.path.join(sar_path, sar_file),
            'st_mtime': os.stat(os.path.join(sar_path, sar_file)).st_mtime
        })

    # Only the 7 most recent ASCII files (no binary saXX files)
    test_by_name = lambda x: re.match(r'^sar[0-9]+$', x['name'])
    sar_filtered_files = filter(test_by_name, sar_files)
    key_by_mtime = lambda x: x['st_mtime']
    sar_filtered_files = sorted(sar_filtered_files, key=key_by_mtime, reverse=True)
    sar_filtered_files = list(sar_filtered_files)

    sections = []
    for sar_file in sar_filtered_files[0:7]:
        with open(sar_file['path'], "r") as sar_open_file:
            sections = parse_file_content(sar_open_file.readlines())
    print(json.dumps(isolate_desired_fields(sections, ['rtps', 'wtps', 'rxkB/s', 'txkB/s', 'await', '%idle', '%commit', 'kbcommit'])))

